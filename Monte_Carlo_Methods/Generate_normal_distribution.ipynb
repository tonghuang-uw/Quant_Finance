{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Normal Random Variables\n",
    "\n",
    "\n",
    "It's important to notice that if $X\\sim N(0, 1)$, then\n",
    "\n",
    "$$\\mu + \\sigma X\\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "This means that we can focus on methods for generating standard normal random variables and then use a simple linear transformation to obtain arbitrary normal distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Box-Muller Algorithm\n",
    "\n",
    "One of the simplest and most popular methods for generating normal random variables is the Box-Muller algorithm.  This method generates two i.i.d. random variables at once by exploiting some symmetries of the normal distribution in polar coordinates.\n",
    "\n",
    "Suppose $X, Y\\sim N(0, 1)$ are i.i.d. standard normal random variables.  Their joint pdf is therefore\n",
    "\n",
    "$$f_{XY}(x, y) = \\frac{1}{2\\pi}e^{-(x^2 + y^2)/2}$$\n",
    "\n",
    "Now define the new variables $V$ and $W$ such that\n",
    "\n",
    "$$X = \\sqrt{V}\\cos W \\:\\textrm{ and }\\: Y = \\sqrt{V}\\sin W$$\n",
    "\n",
    "(These are almost polar coordinates, but we are using $V = R^2$ and $W = \\Theta$ instead of the usual $R$ and $\\Theta$.)\n",
    "\n",
    "The joint pdf of $V$ and $W$ is therefore\n",
    "\n",
    "$$f_{VW}(v, w) = \\frac{1}{4\\pi}e^{-v/2}$$\n",
    "\n",
    "(You should prove this as an exercise.  Notice that you can't just substitute $x = \\sqrt{v}\\cos(w)$ and $y = \\sqrt{v}\\sin(w)$ into $f_{XY}(x, y)$.  You also need to find the Jacobian of the change of variables, which contributes an extra factor of $1/2$.)\n",
    "\n",
    "This joint distribution factors into\n",
    "\n",
    "$$f_{VW}(v, w) = f_{V}(v)f_{W}(w) = \\frac{1}{2}e^{-v/2}\\cdot \\frac{1}{2\\pi}$$\n",
    "\n",
    "and so we know that $V$ and $W$ are independent and $V\\sim \\textrm{Exp}(1/2)$ and $W\\sim U(0, 2\\pi)$.  The Box-Muller algorithm just uses this idea in reverse:\n",
    "\n",
    "1) Generate $V\\sim \\textrm{Exp}(1/2)$ and $W\\sim U(0, 2\\pi)$.\n",
    "\n",
    "2) Set $X = \\sqrt{V}\\cos(W)$ and $Y = \\sqrt{V}\\sin(W)$.\n",
    "\n",
    "This method requires two i.i.d. uniform random variables (one to generate $V$ with the inverse transform method and another to generate $W$) and produces two i.i.d. normal random variables.  It therefore only requires one uniform random variable per normal \n",
    "random variable.\n",
    "\n",
    "(This method is reasonably efficient and quite simple to implement, so it is used in many programming libraries.  In particular, this is how numpy generates normally distributed random variables.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Marsaglia-Bray Algorithm\n",
    "\n",
    "The Box-Muller algorithm requires calculating a sine and a cosine.  On many processors, this is by far the slowest part of the algorithm.  The Marsaglia-Bray algorithm is a modification using acceptance-rejection that avoids the use of trigonometric functions.\n",
    "\n",
    "The key idea of this algorithm is very similar to the previous version.  Suppose that $x$ and $y$ are uniformly distributed on the unit disc.  That is,\n",
    "\n",
    "$$f_{XY}(x, y) = \\left\\{ \\begin{array}{cc} 1/\\pi & x^2 + y^2 \\leq 1 \\\\ 0 & \\textrm{otherwise} \\end{array} \\right.$$\n",
    "\n",
    "Now define random variables $U$ and $V$ such that $X = \\sqrt{U}\\cos(V)$ and $Y = \\sqrt{U}\\sin(V)$.  The joint pdf of $U$ and $V$ is\n",
    "\n",
    "$$f_{UV}(u, v) = \\left\\{ \\begin{array}{cc} \\frac{1}{2\\pi} & (u, v)\\in [0, 1]\\times [0, 2\\pi] \\\\ 0 & \\textrm{otherwise} \\end{array} \\right.$$\n",
    "\n",
    "This means that $U$ and $V$ are uniformly distributed on the box $[0, 1]\\times [0, 2\\pi]$ and therefore $U\\sim U(0, 1)$ and $V\\sim U(0, 2\\pi)$ are independent.\n",
    "\n",
    "Therefore, if we choose a point $(X, Y)$ uniformly on the unit disc, then its angle $\\Theta = \\arctan(X/Y)$ will be uniformly distributed on $[0, 2\\pi]$, which is what we want for the Box-Muller method, but its radius will also be distributed uniformly, which is not what we want.  We can fix this by using another inverse transform.  If $U = X^2 + Y^2 \\sim U(0, 1)$, then $R^2 = -2\\ln U\\sim \\textrm{Exp}(1/2)$.  We therefore have the following algorithm:\n",
    "\n",
    "1) Generate $(X, Y)$ uniformly on the unit circle.  This is easily accomplished with acceptance-rejection.  We can generate i.i.d. $U_1, U_2\\sim U(-1, 1)$ and accept them if $U = U_1^2 + U_2^2 \\leq 1$.\n",
    "\n",
    "2) Set $Z_1 = X\\sqrt{-2\\ln(U)/U}$ and $Z_2 = Y\\sqrt{-2\\ln(U)/U}$.\n",
    "\n",
    "The random variables $Z_1$ and $Z_2$ are i.i.d. standard normal random variables.\n",
    "\n",
    "The Marsaglia-Bray algorithm avoids computing any sines or cosines, but at the expense of using an acceptance-rejection algorithm, which will often require multiple trials to generate one (pair of) normally distributed value(s).  Whether Box-Muller or Marsaglia-Bray is a better choice will depend on the details of both your problem and your processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Normal Random Variables\n",
    "\n",
    "We now have several ways to generate independent normal distributions (with arbitrary mean and variance).  Now we will try to generate non-independent normally distributed random variables.\n",
    "\n",
    "In general, specifying even two non-independent distributions requires fully specifying the joint distribution.  However, if every linear combination of the $X_i$ is normally distributed (which, in particular, means that the marginal distributions of each $X_i$ are normal), then the joint distribution can be fully specified by giving the means and covariances of all the random variables.  That is, if\n",
    "\n",
    "$$X = \\left(X_1, X_2, \\dotsc, X_n\\right)^{T}$$\n",
    "\n",
    "is a random vector where each $Y = \\sum_{i=1}^{n}a_iX_i$ is normally distributed (such an $X$ is known as a *multivariate normal* random variable), then we can fully specify the distribution of $X$ by giving a vector $\\mu\\in\\mathbb{R}^{n}$ and a matrix $\\Sigma\\in\\mathbb{R}^{n\\times n}$, where\n",
    "\n",
    "$$\\mu_i = \\mathbb{E}\\left[X_i\\right] \\:\\textrm{ and }\\: \\Sigma_{ij} = \\textrm{Cov}\\left[X_i, X_j\\right]$$\n",
    "\n",
    "We write $X\\sim N(\\mu, \\Sigma)$ to denote such a distribution.\n",
    "\n",
    "The matrix $\\Sigma$ is known as the *covariance matrix* of $X$ and it has some important properties.  In particular,\n",
    "\n",
    "1) The covariance is symmetric.  That is, $\\Sigma^{T} = \\Sigma$.  This is because $\\textrm{Cov}\\left[X_i, X_j\\right] = \\textrm{Cov}\\left[X_j, X_i\\right]$.\n",
    "\n",
    "2) The diagonal elements are non-negative.  That is, $\\Sigma_{ii} \\geq 0$.  This is because $\\textrm{Cov}\\left[X_i, X_i\\right] = \\textrm{Var}\\left[X_i\\right]$.\n",
    "\n",
    "3) It is positive semi-definite.  That is, $\\mathbf{x}^{T}\\Sigma\\mathbf{x} \\geq 0$ for all $\\mathbf{x}\\in\\mathbb{R}^{n}$.  Equivalently, all of the eigenvalues of $\\Sigma$ are non-negative.  It's worth noting that $\\Sigma$ is positive definite (so that both \"non-negative\" can be replaced with \"positive\" in both statements) if and only if it has full rank.\n",
    "\n",
    "One can show that the if $X$ is a multivariate normal random variable with mean vector $\\mu$ and invertible covariance matrix $\\Sigma$, then the joint pdf is given by\n",
    "\n",
    "$$f_{X}(\\mathbf{x}) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}\\exp\\left(-\\frac{1}{2}\\left(\\mathbf{x} - \\mu\\right)^{T}\\Sigma^{-1}\\left(\\mathbf{x} - \\mu\\right)\\right)$$\n",
    "\n",
    "(If $\\Sigma$ is not invertible, then the joint distribution does not have a pdf.  In this case, life becomes much more difficult.)\n",
    "\n",
    "In addition, linear combinations of multivariate normal random variables are still multivariate normals.  In particular, if $A\\in\\mathbb{R}^{n\\times n}$ and $b\\in\\mathbb{R}^{n}$, then\n",
    "\n",
    "$$Y = AX + \\mathbf{b}$$\n",
    "\n",
    "is also a multivariate normal.  In order to specify the distribution of $Y$, we just have to know its mean and covariance matrix.  Fortunately, we know (for any random vector)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[AX + \\mathbf{b}\\right] &= A\\mathbb{E}\\left[X\\right] + \\mathbf{b} \\\\\n",
    "\\textrm{Cov}\\left[AX + \\mathbf{b}\\right] &= A\\textrm{Cov}\\left[X\\right]A^{T}\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that if $Z\\sim N(0, I_n)$ (that is, $Z$ is a random vector of $n$ i.i.d. standard normal random variables), then\n",
    "\n",
    "$$X = \\mu + AZ \\sim N(\\mu, AA^{T})$$\n",
    "\n",
    "This gives us a way to generate $X\\sim N(\\mu, \\Sigma)$:\n",
    "\n",
    "1) Generate $Z$ by generating $n$ i.i.d. standard normal random variables (for instance, with Box-Muller).\n",
    "\n",
    "2) Determine the matrix $A$ such that $AA^{T} = \\Sigma$.\n",
    "\n",
    "3) Set $X = \\mu + AZ$.\n",
    "\n",
    "The only potentially difficult part here is step (2).  Here, we will appeal to some useful theorems from linear algebra.\n",
    "\n",
    "First, if $\\Sigma$ is a symmetric, positive-definite matrix, then it can be written as\n",
    "\n",
    "$$\\Sigma = LDL^{T}$$\n",
    "\n",
    "where $L$ is a lower triangular matrix and $D$ is a diagonal matrix with non-negative entries.  This means that we can write\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\Sigma &= \\left(L\\sqrt{D}\\right)\\left(\\sqrt{D}L^{T}\\right) \\\\\n",
    "&= \\left(L\\sqrt{D}\\right)\\left(L\\sqrt{D}\\right)^{T}\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $\\sqrt{D}$ is the diagonal matrix whose entries are the square roots of the entries of $D$.  This means that if we set $A = L\\sqrt{D}$, then we have\n",
    "\n",
    "$$\\Sigma = AA^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Generate a multivariate normal random vector $X = (X_1, X_2)$ with means and covariance\n",
    "\n",
    "$$\\mu = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\:\\textrm{ and }\\: \\Sigma = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1.5 \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_1 = 0.8719976232803976, X_2 = 1.971061935994259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_multivariate_normal(mu, A):\n",
    "    z = np.random.randn(2).reshape((2, 1))\n",
    "    return mu + A @ z\n",
    "\n",
    "mu = np.array([[1], [2]])\n",
    "Sigma = np.array([[1, 0.5], [0.5, 1.5]])\n",
    "A = np.linalg.cholesky(Sigma)\n",
    "x = generate_multivariate_normal(mu, A)\n",
    "\n",
    "print(\"X_1 = {}, X_2 = {}\".format(x[0,0], x[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 64-bit",
   "name": "python3117jvsc74a57bd061144442c5bba5d28851d4d88bd107affeefd4f9a3e21babceae053099746efa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}